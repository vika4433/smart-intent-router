# src/mcp_server/server.py
from mcp.server.fastmcp import FastMCP
from intent_classifier.intent_classifier import classify_intent
from language_detector.language_detector import detect_language
from llm_selector.llm_selector import select_llm_model
from lm_studio_proxy.lm_studio_proxy import send_to_lm_studio
from response_handler.response_handler import handle_response

mcp = FastMCP("SmartIntentRouter")

@mcp.tool(title="Route Request")
async def route_request(text: str) -> dict:
    """
    Route a user request to the appropriate LLM based on detected intent and language.

    This tool detects the input language and classifies the user's intent,
    then selects the best matching large language model (LLM) using the configured routing logic.
    The request is sent to LM Studio for processing, and the response is formatted and returned to the client.

    Args:
        text (str): The user's input or prompt.

    Returns:
        dict: The response generated by the selected LLM, or an error message if no suitable model is found.
    """
    language = detect_language(text)
    intent = classify_intent(text)
    model_info = select_llm_model(intent, language)  # Now returns a dict or None

    if not model_info:
        # No suitable model found; return an error response (customize as needed)
        return {"error": "No suitable LLM model found for this intent and language."}

    model_name = model_info["model_name"]
    endpoint = model_info["endpoint"]
    raw_response = send_to_lm_studio(model_name, text, endpoint)
    return handle_response(raw_response, model_name, intent, language)


def run_server():
    mcp.run(host="0.0.0.0", port=8080, streamable=True)
